{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Deep Q learning for Frozen lake using pytorch.\n",
        "Tasks:\n",
        "* Complete the code by replacing all instances of \"?\"\n",
        "and \"your code here\" with your implementation.\n",
        "* Respond to the questions provided at the end of this file."
      ],
      "metadata": {
        "id": "Og9noXznXDRq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import deque\n",
        "import random\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "# Define model\n",
        "class DQN(nn.Module):\n",
        "    def __init__(self, in_states, h1_nodes, out_actions):\n",
        "        super().__init__()\n",
        "\n",
        "        # Define network layers\n",
        "        self.fc1 = nn.Linear(in_states, h1_nodes)   # first fully connected layer\n",
        "        self.out = nn.Linear(h1_nodes, out_actions) # ouptut layer w\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x)) # Apply rectified linear unit (ReLU) activation\n",
        "        x = self.out(x)         # Calculate output\n",
        "        return x\n",
        "\n",
        "# Define memory for Experience Replay\n",
        "class ReplayMemory():\n",
        "    def __init__(self, maxlen):\n",
        "        self.memory = deque([], maxlen=maxlen)\n",
        "\n",
        "    def append(self, transition):\n",
        "        self.memory.append(transition)\n",
        "\n",
        "    def sample(self, sample_size):\n",
        "        return random.sample(self.memory, sample_size)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)\n",
        "\n",
        "# FrozeLake Deep Q-Learning\n",
        "class FrozenLakeDQL():\n",
        "    # Hyperparameters (adjustable)\n",
        "    learning_rate_a = 0.001         # learning rate (alpha)\n",
        "    discount_factor_g = 0.9         # discount rate (gamma)\n",
        "    network_sync_rate = 10          # number of steps the agent takes before syncing the policy and target network\n",
        "    replay_memory_size = 1000       # size of replay memory\n",
        "    mini_batch_size = 32            # size of the training data set sampled from the replay memory\n",
        "\n",
        "    # Neural Network\n",
        "    loss_fn = nn.MSELoss()          # NN Loss function. MSE=Mean Squared Error\n",
        "    optimizer = None                # NN Optimizer. Initialize later.\n",
        "\n",
        "    ACTIONS = ['L','D','R','U']     # for printing 0,1,2,3 => L(eft),D(own),R(ight),U(p)\n",
        "\n",
        "\n",
        "    # Optimize policy network\n",
        "    def optimize(self, mini_batch, policy_dqn, target_dqn):\n",
        "\n",
        "        # Get number of input nodes\n",
        "        num_states = policy_dqn.fc1.in_features\n",
        "\n",
        "        current_q_list = []\n",
        "        target_q_list = []\n",
        "\n",
        "        for state, action, new_state, reward, terminated in mini_batch:\n",
        "\n",
        "            if terminated:\n",
        "                # Agent either reached goal (reward=1) or fell into hole (reward=0)\n",
        "                # When in a terminated state, target q value should be set to the reward.\n",
        "                target = torch.FloatTensor([reward])\n",
        "            else:\n",
        "                # Calculate target q value\n",
        "                with torch.no_grad():\n",
        "                    target = torch.FloatTensor(\n",
        "                        reward + self.discount_factor_g * target_dqn(self.state_to_dqn_input(new_state, num_states)).max()\n",
        "                    )\n",
        "\n",
        "            # Get the current set of Q values\n",
        "            current_q = policy_dqn(self.state_to_dqn_input(state, num_states))\n",
        "            current_q_list.append(current_q)\n",
        "\n",
        "            # Get the target set of Q values\n",
        "            target_q = target_dqn(self.state_to_dqn_input(state, num_states))\n",
        "            # Adjust the specific action to the target that was just calculated\n",
        "            target_q[action] = target\n",
        "            target_q_list.append(target_q)\n",
        "\n",
        "        # Compute loss for the whole minibatch\n",
        "        loss = self.loss_fn(torch.stack(current_q_list), torch.stack(target_q_list))\n",
        "\n",
        "        # Optimize the model\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "    '''\n",
        "    Converts an state (int) to a tensor representation.\n",
        "    For example, the FrozenLake 4x4 map has 4x4=16 states numbered from 0 to 15.\n",
        "\n",
        "    Parameters: state=1, num_states=16\n",
        "    Return: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
        "    '''\n",
        "    def state_to_dqn_input(self, state:int, num_states:int)->torch.Tensor:\n",
        "        input_tensor = torch.zeros(num_states)\n",
        "        input_tensor[state] = 1\n",
        "        return input_tensor\n",
        "\n",
        "\n",
        "    # Print DQN: state, best action, q values\n",
        "    def print_dqn(self, dqn):\n",
        "        # Get number of input nodes\n",
        "        num_states = dqn.fc1.in_features\n",
        "        best_actions=[]\n",
        "\n",
        "        symbol_map = {'L': '←', 'D': '↓', 'R': '→', 'U': '↑'}\n",
        "        # Loop each state and print policy to console\n",
        "        for s in range(num_states):\n",
        "            #  Format q values for printing\n",
        "            q_values = ''\n",
        "            for q in dqn(self.state_to_dqn_input(s, num_states)).tolist():\n",
        "                q_values += \"{:+.2f}\".format(q)+' '  # Concatenate q values, format to 2 decimals\n",
        "            q_values=q_values.rstrip()              # Remove space at the end\n",
        "\n",
        "            # Map the best action to L D R U\n",
        "            best_action = self.ACTIONS[dqn(self.state_to_dqn_input(s, num_states)).argmax()]\n",
        "            best_actions.append(best_action)\n",
        "\n",
        "            # Print policy in the format of: state, action, q values\n",
        "            # The printed layout matches the FrozenLake map.\n",
        "            print(f'{s:02},{best_action},[{q_values}]', end=' ')\n",
        "            if (s+1)%4==0:\n",
        "                print() # Print a newline every 4 states\n",
        "        mapped_results = [symbol_map[action] for action in best_actions]\n",
        "        print(mapped_results)\n",
        "        grid_size = int(np.sqrt(num_states))\n",
        "        policy_grid = np.array(mapped_results).reshape((grid_size, grid_size))\n",
        "\n",
        "        print(\"Optimal Policy:\")\n",
        "        for row in policy_grid:\n",
        "            print(\" \".join(row))\n",
        "\n",
        "        plt.figure(figsize=(5, 5))\n",
        "        plt.table(cellText=policy_grid, cellLoc='center', loc='center', cellColours=[['w'] * grid_size] * grid_size)\n",
        "        plt.axis('off')\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "\n",
        "    # Train the FrozeLake environment\n",
        "    def train(self, episodes, render=False, is_slippery=False):\n",
        "        # Create FrozenLake instance\n",
        "        env = gym.make('FrozenLake-v1', map_name=\"4x4\", is_slippery=is_slippery, render_mode='human' if render else None)\n",
        "\n",
        "        num_states = ?\n",
        "        num_actions = ?\n",
        "\n",
        "        epsilon = 1 # 1 = 100% random actions\n",
        "        memory = ReplayMemory(self.replay_memory_size)\n",
        "\n",
        "        # Create policy and target network. Number of nodes in the hidden layer can be adjusted.\n",
        "        policy_dqn = DQN(in_states=num_states, h1_nodes=num_states, out_actions=num_actions)\n",
        "        target_dqn = DQN(in_states=num_states, h1_nodes=num_states, out_actions=num_actions)\n",
        "\n",
        "        # Make the target and policy networks the same (copy weights/biases from one network to the other)\n",
        "        target_dqn.load_state_dict(policy_dqn.state_dict())\n",
        "\n",
        "        print('Policy (random, before training):')\n",
        "        self.print_dqn(policy_dqn)\n",
        "\n",
        "        # Policy network optimizer. \"Adam\" optimizer can be swapped to something else.\n",
        "        self.optimizer = torch.optim.Adam(policy_dqn.parameters(), lr=self.learning_rate_a)\n",
        "\n",
        "        # List to keep track of rewards collected per episode. Initialize list to 0's.\n",
        "        rewards_per_episode = np.zeros(episodes)\n",
        "\n",
        "        # List to keep track of epsilon decay\n",
        "        epsilon_history = []\n",
        "\n",
        "        # Track number of steps taken. Used for syncing policy => target network.\n",
        "        step_count=0\n",
        "\n",
        "        for i in range(episodes):\n",
        "            state = ?  # Initialize to state 0\n",
        "            terminated = False      # True when agent falls in hole or reached goal\n",
        "            truncated = False       # True when agent takes more than 200 actions\n",
        "\n",
        "            # Agent navigates map until it falls into hole/reaches goal (terminated), or has taken 200 actions (truncated).\n",
        "            while(not terminated and not truncated):\n",
        "\n",
        "                # Select action based on epsilon-greedy\n",
        "                if ?\n",
        "                else:\n",
        "                    # select best action\n",
        "                    with torch.no_grad():\n",
        "                        action = policy_dqn(self.state_to_dqn_input(state, num_states)).argmax().item()\n",
        "\n",
        "                # Execute action in the env\n",
        "                your code here\n",
        "\n",
        "                # Save experience into memory\n",
        "                memory.append((state, action, new_state, reward, terminated))\n",
        "\n",
        "                # Move to the next state\n",
        "                your code here\n",
        "\n",
        "                # Increment step counter\n",
        "                step_count+=1\n",
        "\n",
        "            # Keep track of the rewards collected per episode.\n",
        "            if reward == 1:\n",
        "                rewards_per_episode[i] = 1\n",
        "\n",
        "            # Check if enough experience has been collected and if at least 1 reward has been collected\n",
        "            if len(memory)>self.mini_batch_size and np.sum(rewards_per_episode)>0:\n",
        "                mini_batch = memory.sample(self.mini_batch_size)\n",
        "                self.optimize(mini_batch, policy_dqn, target_dqn)\n",
        "\n",
        "                # Decay epsilon\n",
        "                epsilon = max(epsilon - 1/episodes, 0)\n",
        "                epsilon_history.append(epsilon)\n",
        "\n",
        "                # Copy policy network to target network after a certain number of steps\n",
        "                if step_count > self.network_sync_rate:\n",
        "                    target_dqn.load_state_dict(policy_dqn.state_dict())\n",
        "                    step_count=0\n",
        "\n",
        "        # Close environment\n",
        "        env.close()\n",
        "\n",
        "        # Save policy\n",
        "        torch.save(policy_dqn.state_dict(), \"frozen_lake_dql.pt\")\n",
        "\n",
        "\n",
        "\n",
        "    # Run the FrozeLake environment with the learned policy\n",
        "    def test(self, episodes, is_slippery=False):\n",
        "        # Create FrozenLake instance\n",
        "        env = gym.make('FrozenLake-v1', map_name=\"4x4\", is_slippery=is_slippery, render_mode=None)\n",
        "        num_states = ?\n",
        "        num_actions = ?\n",
        "\n",
        "        # Load learned policy\n",
        "        policy_dqn = DQN(in_states=num_states, h1_nodes=num_states, out_actions=num_actions)\n",
        "        policy_dqn.load_state_dict(torch.load(\"frozen_lake_dql.pt\"))\n",
        "        policy_dqn.eval()    # switch model to evaluation mode\n",
        "\n",
        "        print('Policy (trained):')\n",
        "        self.print_dqn(policy_dqn)\n",
        "\n",
        "        for i in range(episodes):\n",
        "            state = ?  # Initialize to state 0\n",
        "            terminated = False      # True when agent falls in hole or reached goal\n",
        "            truncated = False       # True when agent takes more than 200 actions\n",
        "\n",
        "            # Agent navigates map until it falls into a hole (terminated), reaches goal (terminated), or has taken 200 actions (truncated).\n",
        "            while(not terminated and not truncated):\n",
        "                # Select best action\n",
        "                with torch.no_grad():\n",
        "                    action = policy_dqn(self.state_to_dqn_input(state, num_states)).argmax().item()\n",
        "\n",
        "                # Execute action\n",
        "                your code here\n",
        "\n",
        "        env.close()\n",
        "\n"
      ],
      "metadata": {
        "id": "SlkKKUbwXCCN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "frozen_lake = FrozenLakeDQL()\n",
        "is_slippery = False\n",
        "frozen_lake.train(1000, is_slippery=is_slippery)\n",
        "frozen_lake.test(10, is_slippery=is_slippery)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "C2b0zjZcHVdu",
        "outputId": "06f01d98-2450-4aa1-d414-cc4a30a2550f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Policy (random, before training):\n",
            "00,U,[+0.11 +0.19 -0.23 +0.31] 01,U,[+0.27 +0.20 -0.25 +0.34] 02,U,[+0.30 +0.18 -0.29 +0.31] 03,U,[+0.33 +0.20 -0.26 +0.39] \n",
            "04,D,[+0.12 +0.19 -0.07 +0.19] 05,U,[+0.21 +0.16 -0.16 +0.36] 06,U,[+0.19 +0.22 -0.32 +0.29] 07,U,[+0.14 +0.21 -0.16 +0.22] \n",
            "08,U,[+0.08 +0.12 -0.03 +0.19] 09,U,[+0.12 +0.10 -0.12 +0.19] 10,U,[+0.17 +0.22 -0.25 +0.28] 11,U,[+0.23 +0.17 -0.16 +0.23] \n",
            "12,U,[+0.09 +0.22 -0.24 +0.30] 13,U,[+0.16 +0.17 -0.24 +0.41] 14,U,[+0.19 +0.17 -0.24 +0.22] 15,U,[+0.18 +0.27 -0.27 +0.32] \n",
            "['↑', '↑', '↑', '↑', '↓', '↑', '↑', '↑', '↑', '↑', '↑', '↑', '↑', '↑', '↑', '↑']\n",
            "Optimal Policy:\n",
            "↑ ↑ ↑ ↑\n",
            "↓ ↑ ↑ ↑\n",
            "↑ ↑ ↑ ↑\n",
            "↑ ↑ ↑ ↑\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 500x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Policy (trained):\n",
            "00,D,[+0.49 +0.59 +0.59 +0.55] 01,R,[+0.54 +0.06 +0.65 +0.58] 02,D,[+0.60 +0.73 +0.54 +0.68] 03,L,[+0.66 +0.11 +0.49 +0.60] \n",
            "04,D,[+0.59 +0.65 +0.01 +0.52] 05,U,[+0.57 +0.54 +0.70 +0.72] 06,D,[+0.04 +0.81 -0.01 +0.57] 07,D,[+0.56 +0.58 +0.53 +0.50] \n",
            "08,R,[+0.67 -0.01 +0.73 +0.58] 09,R,[+0.64 +0.81 +0.81 +0.02] 10,D,[+0.73 +0.90 +0.01 +0.73] 11,R,[+0.67 +0.44 +0.68 +0.54] \n",
            "12,U,[+0.38 +0.65 +0.39 +0.71] 13,R,[+0.17 +0.81 +0.90 +0.63] 14,R,[+0.80 +0.88 +1.00 +0.79] 15,U,[+0.51 +0.55 +0.48 +0.71] \n",
            "['↓', '→', '↓', '←', '↓', '↑', '↓', '↓', '→', '→', '↓', '→', '↑', '→', '→', '↑']\n",
            "Optimal Policy:\n",
            "↓ → ↓ ←\n",
            "↓ ↑ ↓ ↓\n",
            "→ → ↓ →\n",
            "↑ → → ↑\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 500x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Questions:\n",
        "1. What is the purpose of ReplayMemory? What types of information are stored in the replay memory? Why is deque used for implementing it?\n",
        "\n",
        "2. Why do we use a target network in Deep Q-Learning? How is the target network updated in this implementation? What is the purpose of variable \"network_sync_rate\" in the code?\n",
        "\n",
        "3. How are training samples generated for learning? When is an experience added to the replay memory.\n",
        "\n",
        "4. How are batches of training samples selected from the replay memory? Is the sampling random or sequential? Why?\n",
        "\n",
        "5. What is the role of the epsilon-greedy policy? How does epsilon change over time in the code, and why is this important?\n",
        "\n",
        "6. Describe what happens during one training step. How are the Q-values updated using the neural network?"
      ],
      "metadata": {
        "id": "9MLccIUoWYIn"
      }
    }
  ]
}